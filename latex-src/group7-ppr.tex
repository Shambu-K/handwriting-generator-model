% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{style/cvpr}      % To produce the REVIEW version
% \usepackage{style/cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{style/cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Handwriting Generation and Animation with Deep Learning}

\author{Anita Dash\\
MA20BTECH11001\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Dhruv Srikanth\\
EE20BTECH11014\\
\and
Shambhu Prasad Kavir\\
CS20BTECH11045\\
\and
Taha Adeel Mohammed\\
CS20BTECH11052\\
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   Handwriting style transfer for personalized text generation is a fascinating and evolving field that sits at the intersection of computer vision and natural language processing. It has several applications in various domains, ranging from enhancing digital communication and marketing to improving accessibility and education.

   In this project, we aim to create a system that takes a written text prompt and a sample of the handwriting to mimic as the input, and generates realistic animated handwritten text that closely resembles the provided style while maintaining readability and coherence.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
% intro, motivation
Handwriting is a deeply personal and authentic form of expression. It holds a unique place in human communication and self-expression. In this regard, Handwriting Style Transfer can come in handy. A model that generates handwritten text in a particular style can be helpful in the following fields:-
\begin{itemize}
    \item In education, personalized handwritten materials can improve student engagement and retention, offering educators a powerful tool to make learning more effective and enjoyable.
    \item Handwriting style transfer can improve accessibility for individuals with motor disabilities, allowing them to convey their thoughts and emotions through personalized handwriting styles, promoting inclusivity in communication.
    \item Handwritten image generation can also provide additional data to train more accurate general handwriting recognition models 
    \item It can help in the field of forensic analysis such as forgery detection, document authentication etc.
    \item Businesses can leverage personalized handwriting in marketing materials, enhancing customer engagement, and creating memorable brand experiences.
\end{itemize}

% the model
% base model
\section{Problem Statement}
\label{sec: PS}
Implement a deep learning model that can take a text prompt and a sample image of a person's handwriting style as input, and generate the image of the text prompt, along with the temporal sequence of brush strokes, in the specified handwriting style as the output. We want the model to accurately mimic the unique characteristics, nuances, and idiosyncrasies of the provided handwriting style while maintaining the readability and coherence of the generated text. From this generated image, we further require the pen trajectory, i.e. the successive coordinate points for every time step of the brush stroke, to be extracted. This trajectory can then be used to animate the brush stroke in real-time, creating a realistic animated handwritten text.  


\section{Literature Review}
\label{sec: Lit Rev}
There has been a lot of research in the field of handwriting generation and handwriting trajectory recovery. In this section, we discuss the various models and approaches that have been proposed in the literature to solve these problems. We then aim to combine these state-of-the-art techniques to create a model that can generate realistic animated handwritten text in a particular style.


\subsection{GANwriting}
\label{subsec: GANWriting}
Generative Adversarial Networks (GANs) have been succesfully used for generating illusory plausible images in various fields. GANs consist of two neural networks, a generator, and a discriminator, which are trained simultaneously through a competitive process in which both improve iteratively. 

In the paper~\cite{GAN-1}, the authors use a conditional non-recurrent generative adversial (cGAN), to produce realistic handwritten word images. In order to produce these diverse stylized words, the textual content along with the specific wrting style, defined by a latent set of calligraphic attributes, are separately conditioned on the generative model. To train the model and achieve the desired results, the authors used the following three novel techniques:
\begin{itemize}
    \item Three complementary learning objectives, namely adversial loss, style classification loss, and reconstruction loss, are used to train the model. State-of-the-art discriminator, classifier, and word recognizer networks are used to train the model.
    \item Character-based content conditioning is done, allowing to generate any word, without being restricted to a specific vocabulary.
    \item Few-shot calligraphic style conditioning is done to avoide the mode collapse problem.
\end{itemize}


However this model is limited to singular words. In the paper~\cite{GAN-2}, they expand on these ideas to allow variable length  textual input, allowing it to generates entire lines of offline handwriting.



\subsection{Handwriting Transformers}
\label{subsec: HWT}
Earlier Handwriting generative methods process style and features separately. It doesn't encode style content entanglement at a character level. In this paper ~\cite{HWT}. the authors propose a transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global (such as ink width, slant) and local (such as character style, ligatures) writing style patterns. The overall architecture has four components: 
\begin{itemize}
  \item Conditional Generator : Synthesize handwritten text.
  \item Discriminator: Ensures realistic generation of handwriting styles. It is designed to be convolutional in nature
  \item Recognizer: Aids in textual content preservation. It is inspired by CRNN
  \item Style Classifier: Ensures satisfactory transfer of calligraphic styles.
\end{itemize}

In the paper, the focus of the design is in the generator model. To imitate a handwriting style as realistically as possible, This model is designed to learn style content entanglement as well as local and global style patterns.  It is a transformer-based generative network for unconstrained styled handwritten text image generation.  It has two main components an encoder network and a decoder network.  Both the encoder and decoder networks constitute a hybrid design based on convolution and multi-head self-attention networks.

HWT generates realistic styled handwritten text images and significantly outperforms other state-of-the-art models through extensive qualitative, quantitative and human based evaluations. The model also generalizes well to the challenging scenarios where both words and writing style are unseen during training, generating realistic styled handwritten text images  \\
\subsection{Decoupled Style Descriptors} 
\label{subsec: Brush Paper}
~\cite{BRUSH-paper} This paper introduces an approach to online handwriting stroke representation via the Decoupled Style Descriptor(DSD) Model. 

\subsection{Dynamically configurable CRNN}
\label{subsec: CRNN}

\subsection{Handwriting Trajectory Recovery}
\label{subsec: img2stroke}
Temporal information is unavailable when it comes to offline text. An image scanner or a video camera is not capable of extracting information like velocity, pressure, inclination etc. If one is able to recover the stroke trajectory from the static 2D image, then offline text can be viewed as an online text. This paper \cite{image2stroke-1Char} proposes a technique that can predict the probable trajectory of an offline character level image.

The model is inspired from the sequence to sequence model based on encoder-decoder architecture. The model sequentially predicts the data coordinate points of the pen trajectory from the offline character images.The framework of this model consists of mainly two steps.
\begin{itemize}
    \item Extract a sequence feature vector from the offline images using CNN.
    \item  An encoder-decoder LSTM network takes the sequence feature as the input and outputs the required coordinate points.
\end{itemize}

The model is able to find out the correct starting point and extract the correct incoming and outgoing paths from junction points effectively. 


\subsection{Domain-Adversarial Neural Network}
\label{subsec: DANN}
~\cite{DANN} DANN (Domain-Adversarial Neural Network) is an interesting architecture that we believe, could be utilized in the problem of specific-style handwriting generation. DANN is an improvement to any regular network structure due to its parallel branch.

Given a Neural Network performing a downstream task on a given dataset, the output of the penultimate layer is considered the final representation of the input data. On this representation, we apply our last layer which is the downstream task itself but the more interesting part is the learnt representation. We claim that with enough training, this penultimate layer output is the best possible latent space representation of the data distribution possessing the information required to perform a downstream task. The nature of Deep Learning makes it so that we can never truly know the kind of correlations the model has learnt from the input data and how the information is being represented but we know that the information required for the downstream task is somehow contained in the representation. If the existence of a downstream task and its loss function controls the information present in the representation, then if we want to include or exclude further information from the representation, all we need to do is have a parallel branch of the neural network performing that downstream task on the representation. When we consider the loss of this new branch downstream task while updating our representation-learning, the learnt representation now possesses or is independent of information regarding this task.

For example, if we had images of X-Rays of a human body part and we had to determine if the subject is fractured or not, our primary downstream task would be to classify the image as fractured or not. But if we have labels as to what part of the human body is present in the image, then we can ensure that our representation learns features of the image that are independent of whether the subject is a hand or a leg. This can be implemented by having a parallel downstream task on the representation that classifies the image as being an X-Ray of a hand or an X-Ray of a leg. Next, we subtract the loss of this classifier for the representation-learning layers. Thus, the model is forced to get worse at identifying whether the subject in the image is a hand or a leg making its learnt representation independent (but aware) of this information and only focussed on whether the subject is fractured or not.

Thus, this architecture allows us to hand-pick the kind of information we want to be depicted in the model's latent space representation of the data by making the model either improve or become worse at a parallel task involving correlated information. This architecture was originally proposed for use in the biomedical space but we believe that style transfer problems too could benefit from such an architecture due to the requirement of considering handwriting style information and character information distinctly. If we are able to isolate the style information from the character information completely giving the model a better empirical understanding of the handwriting style itself, this may improve our ability to generate better handwriting samples.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{style/ieee_fullname}
\bibliography{references/ppr-references}
}

\end{document}
