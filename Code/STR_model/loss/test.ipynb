{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "#set the random seed\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)   \n",
    "\n",
    "\n",
    "\n",
    "dir_path = '../../../DataSet/IAM-Online/Resized_Dataset/Train/Images/'\n",
    "num_files = len([f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))])\n",
    "print(num_files)\n",
    "\n",
    "def loss_fn(input_seq, target_seq):\n",
    "    '''Loss Function for DTW Loss'''\n",
    "    ''' Input sequence and target sequence are of the form n x 5, where n is the number of points in the sequence and 5 is the dimension of each point representing x, y, time, start_of_stroke (binary), end_of_stroke (binary)'''\n",
    "\n",
    "    n = input_seq.shape[0]\n",
    "    m = target_seq.shape[0]\n",
    "\n",
    "    # Create a matrix to store the accumulated distances\n",
    "    dtw_matrix = torch.zeros((n + 1, m + 1))\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    dtw_matrix[0, 1:] = float('inf')\n",
    "    dtw_matrix[1:, 0] = float('inf')\n",
    "\n",
    "    # Create a matrix to store the optimal warping path\n",
    "    path_matrix = torch.zeros((n + 1, m + 1), dtype=torch.int)\n",
    "\n",
    "    # Calculate the accumulated distances and optimal warping path\n",
    "    cost = torch.cdist(input_seq[:, :2], target_seq[:, :2])  # Pairwise Euclidean distances\n",
    "\n",
    "    min_cost = torch.minimum(dtw_matrix[:-1, 1:], dtw_matrix[1:, :-1])\n",
    "    dtw_matrix[1:, 1:] = cost + torch.minimum(min_cost, dtw_matrix[:-1, :-1])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            min_cost = torch.min(torch.stack([dtw_matrix[i, j+1], dtw_matrix[i+1, j], dtw_matrix[i, j]]))\n",
    "            dtw_matrix[i+1, j+1] = cost[i, j] + min_cost\n",
    "\n",
    "    # for i in range(n):\n",
    "    #     for j in range(m):\n",
    "    #         min_cost = min(dtw_matrix[i, j+1], dtw_matrix[i+1, j], dtw_matrix[i, j])\n",
    "    #         dtw_matrix[i+1, j+1] = cost[i, j] + min_cost\n",
    "\n",
    "     # Update the path matrix based on the minimum cost\n",
    "    path_matrix[1:, 1:][dtw_matrix[1:, :-1] < dtw_matrix[:-1, :-1]] = 2  # Horizontal movement\n",
    "    path_matrix[1:, 1:][dtw_matrix[:-1, 1:] < dtw_matrix[:-1, :-1]] = 1  # Vertical movement\n",
    "    path_matrix[1:, 1:][dtw_matrix[:-1, :-1] <= torch.min(dtw_matrix[1:, :-1], dtw_matrix[:-1, 1:])] = 3  # Diagonal movement\n",
    "\n",
    "    # Calculate the DTW loss as the last element in the matrix\n",
    "    dtw_loss = dtw_matrix[-1, -1]\n",
    "\n",
    "    # Compute the optimal warping path\n",
    "    i, j = n, m\n",
    "    warping_path = [(i, j)]\n",
    "    while i > 1 or j > 1:\n",
    "        if path_matrix[i, j] == 1:\n",
    "            i -= 1  # Vertical movement\n",
    "        elif path_matrix[i, j] == 2:\n",
    "            j -= 1  # Horizontal movement\n",
    "        else:\n",
    "            i -= 1  # Diagonal movement\n",
    "            j -= 1\n",
    "        warping_path.append((i, j))\n",
    "\n",
    "    warping_path.reverse()\n",
    "\n",
    "    # Perform backward propagation to compute gradients\n",
    "    # dtw_loss.backward()\n",
    "\n",
    "    # Retrieve the gradients\n",
    "    # gradients = input_seq.grad\n",
    "\n",
    "    return dtw_loss, warping_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn1(input_seq, target_seq):\n",
    "    '''Loss Function for DTW Loss'''\n",
    "    ''' Input sequence and target sequence are of the form n x 5, where n is the number of points in the sequence and 5 is the dimension of each point representing x, y, time, start_of_stroke (binary), end_of_stroke (binary)'''\n",
    "\n",
    "    n = input_seq.shape[0]\n",
    "    m = target_seq.shape[0]\n",
    "\n",
    "    # Create a matrix to store the accumulated distances\n",
    "    dtw_matrix = torch.zeros((n + 1, m + 1))\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    dtw_matrix[0, 1:] = float('inf')\n",
    "    dtw_matrix[1:, 0] = float('inf')\n",
    "\n",
    "    # Calculate the accumulated distances\n",
    "    cost_matrix = torch.cdist(input_seq[:, :2], target_seq[:, :2])  # Pairwise Euclidean distances\n",
    "    dtw_matrix[1:, 1:] = cost_matrix + torch.min(torch.min(dtw_matrix[:-1, 1:], dtw_matrix[1:, :-1]), dtw_matrix[:-1, :-1])\n",
    "\n",
    "    # Create a matrix to store the optimal warping path\n",
    "    path_matrix = torch.zeros((n + 1, m + 1), dtype=torch.int)\n",
    "\n",
    "    # Update the path matrix based on the minimum cost\n",
    "    path_matrix[1:, 1:][dtw_matrix[1:, :-1] < dtw_matrix[:-1, :-1]] = 2  # Horizontal movement\n",
    "    path_matrix[1:, 1:][dtw_matrix[:-1, 1:] < dtw_matrix[:-1, :-1]] = 1  # Vertical movement\n",
    "    path_matrix[1:, 1:][dtw_matrix[:-1, :-1] <= torch.min(dtw_matrix[1:, :-1], dtw_matrix[:-1, 1:])] = 3  # Diagonal movement\n",
    "\n",
    "    # Calculate the DTW loss as the last element in the matrix\n",
    "    dtw_loss = dtw_matrix[-1, -1]\n",
    "\n",
    "    # Compute the optimal warping path\n",
    "    i, j = n, m\n",
    "    warping_path = [(i, j)]\n",
    "    while i > 1 or j > 1:\n",
    "        if path_matrix[i, j] == 1:\n",
    "            i -= 1  # Vertical movement\n",
    "        elif path_matrix[i, j] == 2:\n",
    "            j -= 1  # Horizontal movement\n",
    "        else:\n",
    "            i -= 1  # Diagonal movement\n",
    "            j -= 1\n",
    "        warping_path.append((i, j))\n",
    "\n",
    "    warping_path.reverse()\n",
    "\n",
    "    # Perform backward propagation to compute gradients\n",
    "    # dtw_loss.backward()\n",
    "\n",
    "    # Retrieve the gradients\n",
    "    # gradients = input_seq.grad\n",
    "\n",
    "    return dtw_loss, warping_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loss function\n",
    "# function to get random images from the dataset\n",
    "img_num1 = random.randint(1, num_files + 1)\n",
    "stroke_path = '../../../DataSet/IAM-Online/Resized_Dataset/Train/Strokes/' + f'stroke_{img_num1}.npy'\n",
    "stroke = np.load(stroke_path)\n",
    "input_seq = torch.from_numpy(stroke).float()\n",
    "inp_seq = input_seq.clone() \n",
    "#multiply an offset of constant value to x and y in input_seq\n",
    "offset = 5\n",
    "inp_seq[:, 0] += offset\n",
    "inp_seq[:, 1] += offset\n",
    "\n",
    "img_num2 = random.randint(1, num_files + 1)\n",
    "stroke_path = '../../../DataSet/IAM-Online/Resized_Dataset/Train/Strokes/' + f'stroke_{img_num2}.npy'\n",
    "stroke = np.load(stroke_path)\n",
    "target_seq = torch.from_numpy(stroke).float()\n",
    "print(input_seq.shape)\n",
    "print(inp_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, path = loss_fn(input_seq, target_seq)\n",
    "#plot input_seq(only x and y coordinates)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(input_seq[:, 0], input_seq[:, 1])\n",
    "plt.show()\n",
    "\n",
    "#plot inp_seq(only x and y coordinates)\n",
    "plt.plot(target_seq[:, 0], target_seq[:, 1])\n",
    "plt.show()\n",
    "print('DTW loss:', loss)\n",
    "print('Optimal Warping Path:', path)\n",
    "#print input_seq size\n",
    "print(f'Input Sequence size:- {input_seq.shape}')\n",
    "print(f'No. of mappings:- {len(path)}')\n",
    "# print('Gradients:', gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method1\n",
    "loss, path = loss_fn1(input_seq, target_seq)\n",
    "#plot input_seq(only x and y coordinates)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(input_seq[:, 0], input_seq[:, 1])\n",
    "plt.show()\n",
    "\n",
    "#plot inp_seq(only x and y coordinates)\n",
    "plt.plot(target_seq[:, 0], target_seq[:, 1])\n",
    "plt.show()\n",
    "print('DTW loss:', loss)\n",
    "print('Optimal Warping Path:', path)\n",
    "#print input_seq size\n",
    "print(f'Input Sequence size:- {input_seq.shape}')\n",
    "print(f'No. of mappings:- {len(path)}')\n",
    "# print('Gradients:', gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soft_dtw import SoftDTW\n",
    "criterion = SoftDTW(gamma=1.0, normalize=True)\n",
    "loss = criterion(input_seq, target_seq)\n",
    "print('DTW loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "x = input_seq[:, :2].numpy()\n",
    "y = target_seq[:, :2].numpy()\n",
    "distance, path = fastdtw(x, y, dist=euclidean)\n",
    "print(distance)\n",
    "#print shape of path\n",
    "print(f'Number of mappings:- {np.array(path)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for DTW-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:- Does not work at the moment, need to fix tensor to np array and dimension issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTWLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.0, normalize=True):\n",
    "        super(DTWLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Compute the loss\n",
    "        distance, path = fastdtw(input[:, :2].numpy(), target[:, :2].numpy(), dist=euclidean)\n",
    "        loss = distance\n",
    "        loss = torch.tensor(loss)\n",
    "\n",
    "        # Return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rivak/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m dtw_loss \u001b[39m=\u001b[39m DTWLoss\u001b[39m.\u001b[39mapply\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Compute DTW loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m loss \u001b[39m=\u001b[39m dtw_loss(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "\u001b[1;32m/home/rivak/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb Cell 10\u001b[0m line \u001b[0;36mDTWLoss.forward\u001b[0;34m(ctx, input, target)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, \u001b[39minput\u001b[39m, target):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     distance, path \u001b[39m=\u001b[39m fastdtw(\u001b[39minput\u001b[39m[:, :\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), target[:, :\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), dist\u001b[39m=\u001b[39meuclidean)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(distance, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/loss/test.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Save the path for use in backward pass\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "class DTWLoss(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target):\n",
    "        # Compute the loss\n",
    "        distance, path = fastdtw(input[:, :2].detach().numpy(), target[:, :2].detach().numpy(), dist=euclidean)\n",
    "        loss = torch.tensor(distance, requires_grad=True)\n",
    "\n",
    "        # Save the path for use in backward pass\n",
    "        ctx.save_for_backward(input, target, torch.tensor(path))\n",
    "\n",
    "        # Return the loss\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors from forward pass\n",
    "        input, target, path = ctx.saved_tensors\n",
    "\n",
    "        # Initialize gradients for input and target\n",
    "        grad_input = torch.zeros_like(input)\n",
    "        grad_target = torch.zeros_like(target)\n",
    "\n",
    "        # Compute gradients for input and target based on the path\n",
    "        for i, j in path:\n",
    "            grad_input[i, :2] += grad_output * (input[i, :2] - target[j, :2])\n",
    "            grad_target[j, :2] += grad_output * (target[j, :2] - input[i, :2])\n",
    "\n",
    "        # Return the gradients\n",
    "        return grad_input, grad_target\n",
    "\n",
    "# Example usage\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "target = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n",
    "\n",
    "# Create input and target tensors using input_sec and target_seq for calculating DTW loss and gradients\n",
    "input = inp_seq.clone()\n",
    "target = target_seq.clone()\n",
    "\n",
    "#activate gradient\n",
    "# input.requires_grad = True\n",
    "# target.requires_grad = True\n",
    "\n",
    "\n",
    "# Create an instance of DTWLoss\n",
    "dtw_loss = DTWLoss.apply\n",
    "\n",
    "# Compute DTW loss\n",
    "loss = dtw_loss(input, target)\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Retrieve gradients\n",
    "gradients_input = input.grad\n",
    "gradients_target = target.grad\n",
    "\n",
    "print(\"Gradients for input:\", gradients_input)\n",
    "print(\"Gradients for target:\", gradients_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
