{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Shambu-K/handwriting-generator-model.git\n",
    "# !pip install fastdtw\n",
    "# %cd /kaggle/working/handwriting-generator-model/Code/STR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from model import STR_Model, STR_Model_Longer_512, STR_Model_Longer_1024\n",
    "from dataset.iam_dataloader import HandwritingDataset\n",
    "from loss.stroke_loss import STR_Loss\n",
    "from util.visualize_progress import visualize_progress, plot_losses\n",
    "from fastdtw import fastdtw\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optimizer, device, epoch=0):\n",
    "    # Setting the model to training mode\n",
    "    model.train() \n",
    "    length = len(train_loader)\n",
    "    # Looping over each batch from the training set \n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), desc=f'Epoch {epoch}', total=length):  \n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  \n",
    "        output = model(data)  \n",
    "        loss = loss_function(output, target) \n",
    "        loss.backward()\n",
    "        # Updating the model parameters\n",
    "        optimizer.step() \n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'   Batch: {batch_idx} | Loss: {loss.item()}')\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "def model_fit(model, train_loader, loss_function, optimizer, scheduler, num_epochs, device, checkpoint, data_path, plot=True):\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        if plot: visualize_progress(model, device, data_path)\n",
    "        print('=====================================================================\\n')\n",
    "        loss = train(model, train_loader, loss_function, optimizer, device, epoch+1)\n",
    "        train_losses.append(loss)\n",
    "        scheduler.step()\n",
    "        if epoch % checkpoint == 0:\n",
    "            model_file = f'./checkpoints/STR_model_{epoch}_{int(loss)}.pth'\n",
    "            torch.save(model.state_dict(), model_file) \n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "# Util functions\n",
    "def set_best_model(model, checkpoint_dir):\n",
    "    ''' Set the model with least loss as the best model. '''\n",
    "    best_loss = 100000\n",
    "    best_model = None\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        if file.endswith('.pth'):\n",
    "            loss = int(file.split('_')[-1].split('.')[0])\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = file\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(torch.load(os.path.join(checkpoint_dir, best_model)))\n",
    "        print(f'Best model: {best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 32 # Archibald it is 32\n",
    "checkpoint_interval = 1\n",
    "learning_rate = 0.001\n",
    "lr_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 43840/43840 [00:32<00:00, 1351.76it/s]\n",
      "Preprocessing data:  90%|████████▉ | 1229/1370 [00:32<00:03, 38.26it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 1.75 MiB is free. Including non-PyTorch memory, this process has 3.80 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 16.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/rivak/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/train.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m root_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../../DataSet/IAM-Online/Resized_Dataset/Train\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m HandwritingDataset(root_dir, batch_size, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/dataset/iam_dataloader.py:26\u001b[0m, in \u001b[0;36mHandwritingDataset.__init__\u001b[0;34m(self, root_dir, batch_size, device, transform)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstroke_filenames \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m([f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstroke_dir) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m---> 26\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess_data()\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/dataset/iam_dataloader.py:58\u001b[0m, in \u001b[0;36mHandwritingDataset.preprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(image, torch\u001b[39m.\u001b[39mTensor), \u001b[39m'\u001b[39m\u001b[39mImage is not a tensor\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     57\u001b[0m     \u001b[39massert\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m60\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImage shape is wrong: \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[i\u001b[39m+\u001b[39mj] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mpad(image, (\u001b[39m0\u001b[39;49m, max_width\u001b[39m-\u001b[39;49mimage\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m]), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m'\u001b[39;49m, value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m j, stroke \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(strokes):\n\u001b[1;32m     60\u001b[0m     \u001b[39massert\u001b[39;00m stroke\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStroke shape is wrong\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 1.75 MiB is free. Including non-PyTorch memory, this process has 3.80 GiB memory in use. Of the allocated memory 3.10 GiB is allocated by PyTorch, and 16.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "root_dir = '../../DataSet/IAM-Online/Resized_Dataset/Train'\n",
    "dataset = HandwritingDataset(root_dir, batch_size, device)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = STR_Model_Longer_512().to(device)\n",
    "set_best_model(model, './checkpoints/')\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n",
    "loss_function = STR_Loss(sos_weight=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id: 34173\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../DataSet/IAM-Online/Resized_Dataset/Train/stroke_34173.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rivak/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/train.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Fitting the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m losses \u001b[39m=\u001b[39m model_fit(model, dataloader, loss_function, optimizer, scheduler, num_epochs, device, checkpoint_interval, root_dir, plot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/rivak/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/train.ipynb Cell 7\u001b[0m line \u001b[0;36mmodel_fit\u001b[0;34m(model, train_loader, loss_function, optimizer, scheduler, num_epochs, device, checkpoint, data_path, plot)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m plot: visualize_progress(model, device, data_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=====================================================================\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rivak/Desktop/Deep%20Learning/handwriting-generator-model/Code/STR_model/train.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, train_loader, loss_function, optimizer, device, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Deep Learning/handwriting-generator-model/Code/STR_model/util/visualize_progress.py:38\u001b[0m, in \u001b[0;36mvisualize_progress\u001b[0;34m(model, device, path)\u001b[0m\n\u001b[1;32m     36\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/image_\u001b[39m\u001b[39m{\u001b[39;00mimg_id\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     37\u001b[0m stroke_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/stroke_\u001b[39m\u001b[39m{\u001b[39;00mimg_id\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 38\u001b[0m stroke \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(stroke_path)\n\u001b[1;32m     39\u001b[0m stroke \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(stroke, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Load the image\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../DataSet/IAM-Online/Resized_Dataset/Train/stroke_34173.npy'"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "losses = model_fit(model, dataloader, loss_function, optimizer, scheduler, num_epochs, device, checkpoint_interval, root_dir, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
