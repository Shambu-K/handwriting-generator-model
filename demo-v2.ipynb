{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taha_adeel/Desktop/Sem-7/Deep Learning/Project/Handwriting-Transformers\n"
     ]
    }
   ],
   "source": [
    "# Run below code to set up the environment\n",
    "import os\n",
    "repo_path = 'Handwriting-Transformers'\n",
    "if not os.path.exists(repo_path) and os.getcwd().split('/')[-1] != repo_path:\n",
    "    !git clone https://github.com/ankanbhunia/Handwriting-Transformers\n",
    "if os.getcwd().split('/')[-1] != repo_path:\n",
    "    %cd Handwriting-Transformers\n",
    "if not os.path.exists('files'): # Get the model and data files\n",
    "    %pip install --upgrade --no-cache-dir gdown\n",
    "    !gdown --id 16g9zgysQnWk7-353_tMig92KsZsrcM6k\n",
    "    !unzip files.zip && rm files.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import time\n",
    "from data.dataset import TextDataset, TextDatasetval\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from models.model import TRGAN\n",
    "from params import *\n",
    "from torch import nn\n",
    "from data.dataset import get_transform\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IAM dataset\n",
    "num_writers = 32 # Number of writers to use\n",
    "dataset_path = '../DataSet/IAM/'\n",
    "iam_data_path = 'files/IAM-32.pickle'\n",
    "model_path = 'files/iam_model.pth'\n",
    "\n",
    "os.makedirs(dataset_path, exist_ok=True) # Create the output folder\n",
    "\n",
    "# Unpickle the IAM dataset\n",
    "with open(iam_data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_data = data['test']\n",
    "    for writer_id, i in zip(train_data, range(num_writers)):\n",
    "        train_data[writer_id] = train_data[writer_id][:20]\n",
    "        os.makedirs(os.path.join(dataset_path, str(writer_id)), exist_ok=True)\n",
    "        for word_id, word in enumerate(train_data[writer_id]):\n",
    "            word['img'].save(os.path.join(dataset_path, str(writer_id), str(word_id)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo variables\n",
    "writer_img_paths = os.listdir(dataset_path)\n",
    "# writer_img_paths = ['../dhruv', '../internet']\n",
    "text = 'Does htis work properly? Maybe something longer so that we can see how it works. Ask it more.'\n",
    "\n",
    "num_examples = 15 # Number of words from style writer to be used\n",
    "batch_size = 8 # Number of results per page (Change in params.py also)\n",
    "output_path = '../demo_output/'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_writer_images(image_path):\n",
    "    '''Opens folder of images and returns a list of word images resized to (Wx32)'''\n",
    "    image_list = []\n",
    "    img_ht = 32\n",
    "    for image_name in os.listdir(image_path):\n",
    "        image = cv2.imread(os.path.join(image_path, image_name))\n",
    "        image = cv2.resize(image, (image.shape[1]*img_ht//image.shape[0], img_ht))\n",
    "        image_list.append((Image.fromarray(image), image_name))\n",
    "    return image_list\n",
    "\n",
    "def get_word_images(paragraph_img_path):\n",
    "    '''Opens a paragraph image and returns a list of word images'''\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def post_process(result_img, threshold=0.8):\n",
    "    '''Remove grey background from words'''\n",
    "    result_img[result_img>threshold] = 1\n",
    "    return result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pickle file for input handwriting style\n",
    "\n",
    "# Create test dictionary\n",
    "test_dataset = {'test': {}}\n",
    "for writer_id, writer_img_path in enumerate(writer_img_paths):\n",
    "    test_dataset['test'][writer_id] = []\n",
    "    for word_img, label in preprocess_writer_images(dataset_path + writer_img_path):\n",
    "        test_dataset['test'][writer_id].append({'img': word_img, 'label': label})\n",
    "\n",
    "# Save the pickle file\n",
    "with open('../DataSet/demo.pickle', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)\n",
    "data_path = '../DataSet/demo.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Loading dataset files...\n",
      "(2) Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha_adeel/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/taha_adeel/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with N02\n",
      "initialize network with N02\n",
      "initialize network with N02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha_adeel/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/taha_adeel/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/iam_model.pth : Model loaded Successfully\n",
      "(3) Loading text content...\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the dataset\n",
    "print ('(1) Loading dataset files...')\n",
    "TextDatasetObjval = TextDatasetval(base_path = data_path, num_examples = num_examples)\n",
    "datasetval = torch.utils.data.DataLoader(\n",
    "            TextDatasetObjval,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True, drop_last=True,\n",
    "            collate_fn=TextDatasetObjval.collate_fn)\n",
    "\n",
    "print ('(2) Loading model...')\n",
    "\n",
    "model = TRGAN()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cuda:0')\n",
    "model.netG.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print (model_path+' : Model loaded Successfully')\n",
    "\n",
    "print ('(3) Loading text content...')\n",
    "text_encode =  [j.encode() for j in text.split(' ')]\n",
    "eval_text_encode, eval_len_text = model.netconverter.encode(text_encode)\n",
    "eval_text_encode = eval_text_encode.to(device).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:08<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output images saved in : ../demo_output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_handwriting_style_imgs = []\n",
    "output_imgs = []\n",
    "\n",
    "# Generate the images\n",
    "for i,data_val in enumerate(tqdm.tqdm(datasetval)): \n",
    "    page_val = model._generate_page(data_val['simg'].to(DEVICE), data_val['swids'], eval_text_encode,eval_len_text)\n",
    "    cv2.imwrite(output_path + 'image' + str(i) + '.png', post_process(page_val)*255)\n",
    "    \n",
    "print ('\\nOutput images saved in : ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the images for fid calculations\n",
    "real_path, fake_path = model.save_images_for_fid_calculation(datasetval, epoch=None, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [02:12<00:00, 77.39it/s]\n",
      "100%|██████████| 6144/6144 [01:21<00:00, 75.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score : 18.643953081175653\n"
     ]
    }
   ],
   "source": [
    "import pytorch_fid.fid_score as fid\n",
    "\n",
    "fid_score = fid.calculate_fid_given_paths([real_path, fake_path], device='cuda', dims=2048, batch_size=1, num_workers=8)\n",
    "print ('FID Score : ' + str(fid_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
